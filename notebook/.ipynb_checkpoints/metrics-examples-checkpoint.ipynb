{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6138dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f49ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "import matplotlib\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score,\\\n",
    "                            roc_curve, precision_recall_curve, roc_auc_score, confusion_matrix\n",
    "from skimage import draw, filters, measure\n",
    "from collections import defaultdict\n",
    "\n",
    "# Imports from FLClab metrics package\n",
    "from metrics.segmentation.commons import iou, dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cff0a9",
   "metadata": {},
   "source": [
    "# Classification, segmentation and semantic segmentation\n",
    "\n",
    "This section presents some of the metrics that can be used in cases of classification, segmentation and semantic segmentation. All of the presented metrics are available in `scikit-learn` (`sklearn`).\n",
    "\n",
    "Note. In case of semantic segmentation the metrics can be reported in a _per class_ fashion or averaged of all classes. It's only a matter of iterating over all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "def show_examples(truth, prediction, instance_objects=False, cmap=\"gray\"):\n",
    "    \"\"\"\n",
    "    Plots the truth and prediction masks\n",
    "    \n",
    "    :param truth: A `numpy.ndarray` of the truth\n",
    "    :param prediction: A `numpy.ndarray` of the prediction\n",
    "    \n",
    "    :returns : A `matplotlib.Figure`\n",
    "               A `matplotlib.Axes`\n",
    "    \"\"\"\n",
    "    fig, axes = pyplot.subplots(1, 2, figsize=(6, 3), tight_layout=True, sharex=True, sharey=True)\n",
    "    \n",
    "    axes[0].imshow(truth, cmap=cmap)\n",
    "    if instance_objects:\n",
    "        vmin, vmax = 0, prediction.max()\n",
    "    else:\n",
    "        vmin, vmax= 0, 1\n",
    "    axes[1].imshow(prediction, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    for ax in axes:\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "    pyplot.show()\n",
    "    return fig, axes\n",
    "\n",
    "def show_tpfpfn(truth, prediction, cmap=\"magma\"):\n",
    "    \"\"\"\n",
    "    Shows True Positive (TP), False Positive (FP) and False Negative (FN) \n",
    "    of the predicted mask.\n",
    "    \n",
    "    :param truth: A `numpy.ndarray` of the truth mask\n",
    "    :param prediction: A `numpy.ndarray` of the predicted mask    \n",
    "    :param cmap: A `string` or `colormap` object to display the results\n",
    "    \n",
    "    :returns : A `matplotlib.Figure`\n",
    "               A `matplotlib.Axes`\n",
    "    \"\"\"\n",
    "    tp = numpy.logical_and(truth, prediction)\n",
    "    fp = numpy.logical_and(numpy.invert(truth > 0), prediction)\n",
    "    fn = numpy.logical_and(truth, numpy.invert(prediction > 0))\n",
    "    mask = numpy.zeros(truth.shape)\n",
    "    mask[tp] = 3\n",
    "    mask[fp] = 2\n",
    "    mask[fn] = 1\n",
    "\n",
    "    fig, ax = pyplot.subplots(figsize=(3,3), tight_layout=True)\n",
    "    if isinstance(cmap, str):\n",
    "        cmap = pyplot.get_cmap(cmap, 4)\n",
    "    im = ax.imshow(mask, cmap=cmap)\n",
    "    cbar = pyplot.colorbar(im, ax=ax)\n",
    "    cbar.set_ticks(numpy.linspace(0.75, 3, 4) - 0.75/2)\n",
    "    cbar.set_ticklabels([\"TN\", \"FN\", \"FP\", \"TP\"])\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "def generate_masks(is_prob_prediction=False, is_smooth=False, instance_objects=False, num_objs=20, prob_prediction=0.9, prob_truth=0.9, seed=42):\n",
    "    \"\"\"\n",
    "    Randomly generates a truth and prediction masks\n",
    "    \n",
    "    :param is_prob_prediction: A `bool` if the prediction is probabilistic\n",
    "    :param is_smooth: A `bool` if the prediction should be smoothed\n",
    "    :param instance_objects: A `bool` if the truth and prediction are instance objects\n",
    "    :param num_objs: An `int` of the number of objects to add\n",
    "    :param prob_prediction: A `float` of the probability that the corresponding prediction is present\n",
    "    :param prob_prediction: A `float` of the probability that the corresponding truth is present\n",
    "    :param seed: An `int` of the random seed\n",
    "    \n",
    "    :returns : A `numpy.ndarray` of the truth mask\n",
    "               A `numpy.ndarray` of the prediction mask\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    numpy.random.seed(seed)\n",
    "\n",
    "    truth = numpy.zeros((256, 256))\n",
    "    prediction =  numpy.zeros((256, 256))\n",
    "    for i in range(num_objs):\n",
    "        center = numpy.random.randint(truth.shape[0], size=(2,))\n",
    "\n",
    "        if random.random() < prob_truth:\n",
    "            rr, cc = draw.disk(center, radius=10, shape=truth.shape)\n",
    "            if instance_objects:\n",
    "                truth[rr, cc] = i + 1\n",
    "            else:\n",
    "                truth[rr, cc] = 1\n",
    "\n",
    "        if random.random() < prob_prediction:\n",
    "            rr, cc = draw.disk(center + numpy.random.randint(10, size=(2, )), radius=numpy.random.randint(8, 15), shape=prediction.shape)\n",
    "            if is_prob_prediction and not instance_objects:\n",
    "                prediction[rr, cc] = random.random()\n",
    "            else:\n",
    "                if instance_objects:\n",
    "                    prediction[rr, cc] = i + 1\n",
    "                else:\n",
    "                    prediction[rr, cc] = 1\n",
    "    if is_smooth and not instance_objects:\n",
    "        prediction = filters.gaussian(prediction, 10)\n",
    "        prediction = prediction / prediction.max()\n",
    "    return truth, prediction\n",
    "\n",
    "def plot_cm(cm, title=None, normalized=True, labels=[0,1], cmap=\"magma\"):\n",
    "    \"\"\"\n",
    "    Plots the confusion matrix on a `matplotlib.Axes`\n",
    "\n",
    "    :param cm: A 2D `numpy.ndarray`\n",
    "    :param normalized: Wheter to normalize the confusion matrix\n",
    "    \"\"\"\n",
    "    tmp = cm.copy()\n",
    "    if normalized:\n",
    "        cm = cm / cm.sum(axis=1)[:, numpy.newaxis]\n",
    "    fig, ax = pyplot.subplots(tight_layout=True, figsize=(3, 3))\n",
    "    im = ax.imshow(cm, cmap=cmap, vmin=0, vmax=1)\n",
    "    cb = fig.colorbar(im, ax=ax)\n",
    "    for j in range(cm.shape[1]):\n",
    "        for i in range(cm.shape[0]):\n",
    "            val = cm[j, i]\n",
    "            counts = int(tmp[j, i])\n",
    "            ax.text(i, j, \"{:d}\".format(counts), horizontalalignment=\"center\",\n",
    "                    verticalalignment=\"center\", fontdict={\"fontsize\":10, \"color\":\"black\" if val < 0.5 else \"white\"})       \n",
    "    ax.set(\n",
    "        title=title,\n",
    "        xticks=numpy.arange(len(labels)),\n",
    "        yticks=numpy.arange(len(labels)),\n",
    "        xticklabels= labels,\n",
    "        yticklabels=labels,\n",
    "        ylabel=\"True label\",\n",
    "        xlabel=\"Predicted label\"\n",
    "    )        \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72677024",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth, prediction = generate_masks()\n",
    "fig, axes = show_examples(truth, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79e9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = show_tpfpfn(truth, prediction)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f9c325",
   "metadata": {},
   "source": [
    "Calculation of common metrics such as: _accuracy_, _precision_, and _recall_. Another way to report the performance of the model is to calculate the Intersection Over Union (IOU) (also called Jaccard index). This metric is commonly used when assessing the segmentation performance of a model. The Dice metric (also called Sørensen–Dice coefficient) is also reported in some cases but is very similar to IOU.\n",
    "\n",
    "In case of multiclass classification, the confusion matrix is a nice way to visually see the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84563e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, metric_func in zip(\n",
    "    [\"Accuracy\", \"Precision\", \"Recall\", \"IOU\", \"Dice\"], \n",
    "    [accuracy_score, precision_score, recall_score, iou, dice]\n",
    "):\n",
    "    print(name, metric_func(truth.ravel(), prediction.ravel()))\n",
    "    \n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\n",
    "    name=\"space\",\n",
    "    colors=[\"#343434\", \"#ffc949\"]\n",
    ")\n",
    "cm = confusion_matrix(truth.ravel(), prediction.ravel())\n",
    "fig, ax = plot_cm(cm, cmap=\"Blues\", labels=[\"Bckg\", \"Fg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efeb5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth, prediction = generate_masks(is_prob_prediction=True, is_smooth=False)\n",
    "fig, axes = show_examples(truth, prediction)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(truth.ravel(), prediction.ravel())\n",
    "fig, ax = pyplot.subplots(figsize=(3,3), tight_layout=True)\n",
    "ax.plot(fpr, tpr, label=\"ROC-curve\")\n",
    "ax.plot(fpr, thresholds, label=\"Threshold\")\n",
    "ax.set(\n",
    "    xlabel=\"FPR\", ylabel=\"TPR\",\n",
    "    ylim=(0, 1), xlim=(0, 1)\n",
    ")\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(truth.ravel(), prediction.ravel())\n",
    "fig, ax = pyplot.subplots(figsize=(3,3), tight_layout=True)\n",
    "ax.plot(recall, precision, label=\"PR-curve\")\n",
    "ax.plot(recall[1:], thresholds, label=\"Threshold\")\n",
    "ax.set(\n",
    "    xlabel=\"Recall\", ylabel=\"Precision\",\n",
    "    ylim=(0, 1), xlim=(0, 1)    \n",
    ")\n",
    "pyplot.legend()\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e809be",
   "metadata": {},
   "source": [
    "## Caveats\n",
    "\n",
    "Accuracy metric is very dependant on the number of negative pixel in the image even if the segmentation is kept constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81129ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    name : func\n",
    "    for name, func in zip([\"Accuracy\", \"Precision\", \"Recall\"], \n",
    "    [accuracy_score, precision_score, recall_score])\n",
    "}\n",
    "scores = defaultdict(list)\n",
    "img_sizes = [32, 64, 128, 256, 512]\n",
    "for img_size in img_sizes:\n",
    "    truth = numpy.zeros((img_size, img_size))\n",
    "    truth[:25, :25] = 1\n",
    "    prediction = numpy.zeros((img_size, img_size))\n",
    "    prediction[:20, :28] = 1\n",
    "    \n",
    "    for name, func in metrics.items():\n",
    "        scores[name].append(func(truth.ravel(), prediction.ravel()))\n",
    "    \n",
    "fig, ax = pyplot.subplots(figsize=(3,3), tight_layout=True)\n",
    "for key, value in scores.items():\n",
    "    ax.plot(img_sizes, value, label=key)\n",
    "ax.set(\n",
    "    ylabel=\"Score\", xlabel=\"Image Size\"\n",
    ")\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f998d03",
   "metadata": {},
   "source": [
    "The IOU metric can be sensitive to the size of the objects in the field of view. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2786b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import transform\n",
    "\n",
    "scores = []\n",
    "radii = [8, 16, 32, 64]\n",
    "for radius in radii:\n",
    "    \n",
    "    truth = numpy.zeros((256, 256))\n",
    "    prediction =  numpy.zeros((256, 256))\n",
    "    \n",
    "    rr, cc = draw.disk((128, 128), radius=radius, shape=truth.shape)\n",
    "    truth[rr, cc] = 1\n",
    "    rr, cc = draw.disk((128, 128), radius=radius - 1, shape=prediction.shape)\n",
    "    prediction[rr, cc] = 1    \n",
    "    \n",
    "    fig, ax = show_tpfpfn(truth, prediction)\n",
    "    pyplot.show()\n",
    "\n",
    "    scores.append(iou(truth, prediction).item())\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize=(3, 3), tight_layout=True)\n",
    "ax.plot(radii, scores)\n",
    "ax.set(\n",
    "    ylabel=\"IOU\", xlabel=\"Object Size\",\n",
    "    xticks=radii\n",
    ")\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afe4694",
   "metadata": {},
   "source": [
    "# Instance segmentation or object detection\n",
    "\n",
    "The general goal of this section is to associate truth and predicted objects in some fashion using some metric.\n",
    "\n",
    "In the instance segmentation the goal is to associate the truth objects with the predicted objects. The instance segmentation is mostly used and seen in cases of cell body segmentation, where all cell bodies in the image should be specific instances of a cell. The metrics that can be used to perform the association can be the `f1-score`, `IOU`, or Bounding Boxes (BBOX).\n",
    "\n",
    "Object detection is quite similar with instance segmentation in the sense that you would like to detect instances of objects. In the object detection task, you may wish to associate centroids of the truth and predicted objects if they are within some distance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa2b160",
   "metadata": {},
   "source": [
    "## Instance Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d1abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\n",
    "    name=\"nice-prism\",\n",
    "    colors=[\"#000000\", \"#5F4690\",\"#1D6996\",\"#38A6A5\",\"#0F8554\",\"#73AF48\",\"#EDAD08\",\"#E17C05\",\"#CC503E\",\"#94346E\"]\n",
    ")\n",
    "\n",
    "truth, prediction = generate_masks(instance_objects=True, num_objs=25, prob_prediction=0.75, prob_truth=0.75)\n",
    "fig, ax = show_examples(truth, prediction, instance_objects=True, cmap=label_cmap)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b9188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.detection import IOUDetectionError\n",
    "\n",
    "scorer = IOUDetectionError(truth.astype(int), prediction.astype(int))\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize=(3,3), tight_layout=True)\n",
    "im = ax.imshow(scorer.cost_matrix, cmap=\"Blues\")\n",
    "cbar = pyplot.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"IOU\")\n",
    "ax.set(\n",
    "    ylabel=\"Truth Objects\", xlabel=\"Predicted Objects\"\n",
    ")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9855e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scorer object allows the user to extract common metrics such as F1-score and \n",
    "# Average Precision as a function of the IOU\n",
    "thresholds = numpy.linspace(0., 1.0, num=10)\n",
    "f1_score, thresholds = scorer.get_f1_score(thresholds)\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize=(3,3), tight_layout=True)\n",
    "ax.plot(thresholds, f1_score)\n",
    "ax.set(\n",
    "    ylim=(0, 1), xlim=(0, 1),\n",
    "    ylabel=\"F1-score\", xlabel=\"IOU\"\n",
    ")\n",
    "pyplot.show()\n",
    "\n",
    "thresholds = numpy.linspace(0., 1.0, num=10)\n",
    "average_precision, thresholds = scorer.get_average_precision(thresholds)\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize=(3,3), tight_layout=True)\n",
    "ax.plot(thresholds, average_precision)\n",
    "ax.set(\n",
    "    ylim=(0, 1), xlim=(0, 1),\n",
    "    ylabel=\"Average Precision\", xlabel=\"IOU\"\n",
    ")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee8dda9",
   "metadata": {},
   "source": [
    "An approach that was described by Caicedo _et al_ [1] is to report the number of merges, splits, extra objects and missed objects as a function of the IOU. This can be done using `IOUDetectionError`.\n",
    "\n",
    "[1] Caicedo, J. C. et al. Evaluation of Deep Learning Strategies for Nucleus Segmentation in Fluorescence Images. Cytometry Part A 95, 952–965 (2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8452995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = numpy.linspace(0, 1, 10)\n",
    "missed_objs, _ = scorer.get_missed_objects(threshold=thresholds)\n",
    "extra_objs, _ = scorer.get_extra_objects(threshold=thresholds)\n",
    "split_objs, _ = scorer.get_split_objects(threshold=thresholds)\n",
    "merged_objs, _ = scorer.get_merged_objects(threshold=thresholds)\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize=(3,3), tight_layout=True)\n",
    "for name, objs in zip([\"Missed\", \"Extra\", \"Split\", \"Merged\"], [missed_objs, extra_objs, split_objs, merged_objs]):\n",
    "    # Objs is a list of list of regionprops\n",
    "    objs = [len(o) for o in objs]\n",
    "    ax.plot(thresholds, objs, label=name)\n",
    "ax.set(\n",
    "    ylabel=\"Num Objects\", xlabel=\"IOU\"\n",
    ")\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde66717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IOUDetectionError has the possibility of showing the performance of the model \n",
    "# extracting merges, splits, extra objects and missed objects\n",
    "fig, axes = scorer.show(threshold=0.10, cmap=label_cmap)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67955aa4",
   "metadata": {},
   "source": [
    "In the Cell Tracking Challenge, the metric that are used to compute the performance of the model are `SEG` and `DET` [1]. This metric is quite challenging to use and requires a specific folder architecture. A tool was created to facilitate the use of this particular metric. \n",
    "\n",
    "**This metric will not use with Windows. If you want to test or make it work on Windows, please share!**\n",
    "\n",
    "<br>\n",
    "<i>\n",
    "[1] Ulman, V. et al. An objective comparison of cell-tracking algorithms. Nature Methods 14, 1141–1152 (2017).\n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ff634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.aogm import CTCMeasure\n",
    "\n",
    "with CTCMeasure([truth], [prediction]) as ctc_measure:\n",
    "    print(\"SEG:\", ctc_measure.get_seg())\n",
    "    print(\"DET:\", ctc_measure.get_det())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514da2e",
   "metadata": {},
   "source": [
    "## Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d776aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.detection import CentroidDetectionError\n",
    "\n",
    "truth_centroids = numpy.array([rprop.centroid for rprop in measure.regionprops(truth.astype(int))])\n",
    "prediction_centroids = numpy.array([rprop.centroid for rprop in measure.regionprops(prediction.astype(int))])\n",
    "\n",
    "scorer = CentroidDetectionError(truth_centroids, prediction_centroids, threshold=25)\n",
    "display(scorer.get_score_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64aaedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_couple, pred_couple = scorer.get_coupled()\n",
    "false_positives = scorer.get_false_positives()\n",
    "false_negatives = scorer.get_false_negatives()\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize=(3,3), tight_layout=True)\n",
    "ax.scatter(\n",
    "    truth_centroids[:, 1], truth_centroids[:, 0], marker=\"+\", label=\"Truth\"\n",
    ")\n",
    "ax.scatter(\n",
    "    prediction_centroids[:, 1], prediction_centroids[:, 0], marker=\"+\", label=\"Prediction\"\n",
    ")\n",
    "for tidx, pidx in zip(truth_couple, pred_couple):\n",
    "    ax.plot(\n",
    "        [truth_centroids[tidx, 1], prediction_centroids[pidx, 1]], \n",
    "        [truth_centroids[tidx, 0], prediction_centroids[pidx, 0]],\n",
    "        color=\"black\"\n",
    "    )\n",
    "ax.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3e9f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CentroidDetectionError can also work in >2D (actually works in ND)\n",
    "# Example where centroids are events from 2D+t movie, e.g. calcium movies\n",
    "truth_centroids = numpy.random.rand(128, 3) * 512\n",
    "prediction_centroids = truth_centroids + numpy.random.normal(loc=0, scale=5, size=truth_centroids.shape)\n",
    "# Centroids are subsampled to simulate missing and extra objects\n",
    "choices = numpy.random.choice(len(truth_centroids), size=125, replace=False)\n",
    "truth_centroids = truth_centroids[choices]\n",
    "choices = numpy.random.choice(len(prediction_centroids), size=115, replace=False)\n",
    "prediction_centroids = prediction_centroids[choices]\n",
    "\n",
    "scorer = CentroidDetectionError(truth_centroids, prediction_centroids, threshold=10)\n",
    "display(scorer.get_score_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c42fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
